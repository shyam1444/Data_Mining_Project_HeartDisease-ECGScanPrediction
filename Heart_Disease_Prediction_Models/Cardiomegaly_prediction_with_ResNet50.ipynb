{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Cardiomegaly prediction with ResNet50",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'cardiomegaly-disease-prediction-using-cnn:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1837618%2F2999507%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240501%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240501T044526Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1852db2b97868f821bdacc78100063ed6edc1af5cd8c8b559e195903193b067234064ddacc7e350f4cee20d33a968f8221b872822da7dd1fd3bf0237d62f90c42654739ecb8ee91c20f981480186eba18ed273e209b3067e792cadbf0e3a88aaf3b9685c1a14bece7be06b11b6f02a6ac5a95e0169fe07206c57c7903275339752c9aaa267781364b256013305112cd818e77aa4e9d37e8235c0177ef48a4fd770ca27d6f088d9879bdad7670d871ffca18944fa1169cea1172a5f2bb735f626728085ee040886981cb01099ff13b4d12f5740e0f5547a025f4c2639e759e9d9813cf979b52e40f62ed23b4edc47e37b4497a2b3e3dfcc879051c7e2c25a3ca3'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "8YodfA_VeNPH"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.initializers import random_uniform, glorot_uniform\n",
        "from matplotlib.pyplot import imshow\n",
        "import cv2 as ocv\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "def load_data(dir, class_names, num_of_img_per_class, input_shape):\n",
        "    classes = class_names\n",
        "    num_of_img_per_class = num_of_img_per_class\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    class_counter = 1\n",
        "    for category in os.listdir(dir):\n",
        "\n",
        "        if class_counter==1:\n",
        "            print(\"Loading Data:\", end=\" \")\n",
        "        new_dir = os.path.join(dir,category)\n",
        "\n",
        "        for img in os.listdir(new_dir):\n",
        "            if len(data)<(num_of_img_per_class * class_counter):\n",
        "                img_path = os.path.join(new_dir,img)\n",
        "                if (np.all(np.array(ocv.imread(img_path,0)))==0):\n",
        "                    data.append(ocv.resize(ocv.imread(img_path,1),(input_shape[0],input_shape[1])))\n",
        "                    labels.append(classes.index(category))\n",
        "                    if len(data)/(num_of_img_per_class*2)*100%20==0:\n",
        "                        print(int(len(data)/(num_of_img_per_class*2)*100),\"%\", end=\"  \")\n",
        "\n",
        "        class_counter=class_counter+1\n",
        "\n",
        "        if class_counter==3:\n",
        "            print(\". Complete.\", end=\" \")\n",
        "\n",
        "    combined = list(zip(data,labels))\n",
        "    random.shuffle(combined)\n",
        "    data, labels = zip(*combined)\n",
        "    Data = np.array(data)/255\n",
        "    Labels = np.array(labels)\n",
        "    n_col = Data.shape[1]\n",
        "    n_rows = Labels.shape[0]\n",
        "    return (Data, Labels)\n",
        "\n",
        "\n",
        "def identity_block(X, f, filters, training=True, initializer=random_uniform):\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    X_shortcut = X\n",
        "\n",
        "    X = Conv2D(filters = F1, kernel_size = 1, strides = (1,1), padding = 'valid', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training = training)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    #Second component of main path\n",
        "    X = Conv2D(filters = F2, kernel_size = f,strides = (1, 1),padding='same',kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training=training)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    #Third component of main path\n",
        "    X = Conv2D(filters = F3, kernel_size = 1, strides = (1, 1), padding='valid', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training=training)\n",
        "\n",
        "    #Adding shortcut value to main path, and pass it through a RELU activation\n",
        "    X = Add()([X_shortcut,X])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def convolutional_block(X, f, filters, s = 2, training=True, initializer=glorot_uniform):\n",
        "    F1, F2, F3 = filters\n",
        "    X_shortcut = X\n",
        "\n",
        "    #First component of main path\n",
        "    X = Conv2D(filters = F1, kernel_size = 1, strides = (s, s), padding='valid', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training=training)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    #Second component of main path\n",
        "    X = Conv2D(filters = F2, kernel_size = f,strides = (1, 1),padding='same',kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training=training)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    #Third component of main path\n",
        "    X = Conv2D(filters = F3, kernel_size = 1, strides = (1, 1), padding='valid', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training=training)\n",
        "\n",
        "    #SHORTCUT PATH\n",
        "    X_shortcut = Conv2D(filters = F3, kernel_size = 1, strides = (s, s), padding='valid', kernel_initializer = initializer(seed=0))(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis = 3)(X_shortcut, training=training)\n",
        "\n",
        "    #Adding shortcut value to main path and passing it through a RELU activation\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def ResNet50(input_shape = (64, 64, 3), classes = 1):\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "    X = ZeroPadding2D((3, 3))(X_input)\n",
        "\n",
        "    #Stage 1\n",
        "    X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
        "\n",
        "    #Stage 2\n",
        "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], s = 1)\n",
        "    X = identity_block(X, 3, [64, 64, 256])\n",
        "    X = identity_block(X, 3, [64, 64, 256])\n",
        "\n",
        "    #Stage 3\n",
        "    X = convolutional_block(X, f = 3, filters = [128,128,512], s = 2)\n",
        "    X = identity_block(X, 3,  [128,128,512])\n",
        "    X = identity_block(X, 3,  [128,128,512])\n",
        "    X = identity_block(X, 3,  [128,128,512])\n",
        "\n",
        "    #Stage 4\n",
        "    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], s = 2)\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "\n",
        "    #Stage 5\n",
        "    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], s = 2)\n",
        "    X = identity_block(X, 3, [512, 512, 2048])\n",
        "    X = identity_block(X, 3, [512, 512, 2048])\n",
        "\n",
        "    #AVGPOOL\n",
        "    X = AveragePooling2D((2, 2))(X)\n",
        "\n",
        "    #Output layer\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(classes, activation='sigmoid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "\n",
        "    #Create model\n",
        "    model = Model(inputs = X_input, outputs = X)\n",
        "\n",
        "    return model\n",
        "\n",
        "dir = '/kaggle/input/cardiomegaly-disease-prediction-using-cnn/train/train'\n",
        "classes = ['false','true']\n",
        "num_of_img_per_class = 2000\n",
        "input_shape = (128, 128, 3)\n",
        "\n",
        "x_train, y_train = load_data(dir, classes, num_of_img_per_class, input_shape)\n",
        "\n",
        "model = ResNet50(input_shape, classes = 1)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs = 20, batch_size = 32)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-04-25T15:11:38.606141Z",
          "iopub.execute_input": "2023-04-25T15:11:38.607005Z",
          "iopub.status.idle": "2023-04-25T15:16:28.927422Z",
          "shell.execute_reply.started": "2023-04-25T15:11:38.606965Z",
          "shell.execute_reply": "2023-04-25T15:16:28.926229Z"
        },
        "trusted": true,
        "id": "Omkunt3ceNPI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}